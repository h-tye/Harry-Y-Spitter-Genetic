# -*- coding: utf-8 -*-
"""BSNN_1x2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/191_xJtI-EyDG7lZ2bDUI-GwMGhATgAEW
"""

# Commented out IPython magic to ensure Python compatibility.
#Beam Splitter Nueral Network

from google.colab import drive
drive.mount('/content/drive')

# Navigate to your folder
# %cd /content/drive/MyDrive/SimData1x2/data_storage/

import os
import numpy as np
from tqdm import tqdm

#define function to load in configuration and associated FOM for every sim
def load_data(folder_path):
    configs, foms = [], []
    txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]

    for filename in tqdm(txt_files, desc="Loading files"):
        try:
            with open(os.path.join(folder_path, filename), 'r') as f:

                lines = f.readlines()

                # Parse 20x20 binary matrix (semicolon-separated rows)
                matrix_lines = lines[0].strip().split(';')[:-1]  # Split rows and remove last empty
                config = []
                for row in matrix_lines:
                    config.append(list(map(int, row.strip().split('\t'))))  # Split each row by tabs

                # Convert to numpy array (20x20)
                config = np.array(config, dtype=np.float32)  # float32 for CNN compatibility

                # Parse FOM (2nd line)
                fom = float(lines[1].strip())

                configs.append(config)
                foms.append(fom)
        except Exception as e:
            print(f"Skipping {filename}: {str(e)[:50]}...")

    return np.stack(configs).reshape(-1, 20, 20, 1), np.array(foms)

#load in data
X, y = load_data("/content/drive/MyDrive/SimData1x2/data_storage/")
print(f"Data shape: {X.shape}")  # Should be (n_files, 20, 20, 1)
print("Sample config (first 5x5 corner):\n", X[0, :5, :5, 0])
print("Sample FOM:", y[0])

from sklearn.model_selection import train_test_split

#split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"X shape: {X_train.shape}")  # Should be (n_samples, 20, 20, 1)
print(f"y shape: {y_train.shape}")  # Should be (n_samples,)

import tensorflow as tf

# Create a dataset pipeline for better performance
dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
dataset = dataset.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.AUTOTUNE)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LeakyReLU, Dropout # Import LeakyReLU
from tensorflow.keras.layers import GaussianNoise

#define model architecture
model = Sequential([
    GaussianNoise(0.01, input_shape=(20, 20, 1)),
    Conv2D(32, (3, 3), activation=LeakyReLU(), input_shape=(20, 20, 1)),
    Conv2D(32, (3, 3), activation=LeakyReLU()),  # Added another Conv2D layer
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation=LeakyReLU()),
    Conv2D(64, (5, 5), activation=LeakyReLU()),  # Added Conv2D with a different filter size
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation=LeakyReLU()),  # Increased the number of neurons
    Dropout(0.5),
    Dense(1)
])

#train and test
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.fit(dataset, epochs=20, validation_data=(X_test, y_test))

#test with new data
import numpy as np

sample_idx = 42  # Pick any index (0 to len(X_train)-1)
known_config = X_train[sample_idx]  # Shape: (20, 20, 1)
known_fom = y_train[sample_idx]     # Actual FOM value

predicted_fom = model.predict(known_config[np.newaxis, ...])[0][0]  # Add batch dimension

print(f"\n=== Sample {sample_idx} ===")
print(f"Actual FOM:    {known_fom:.4f}")
print(f"Predicted FOM: {predicted_fom:.4f}")
print(f"Absolute Error: {abs(predicted_fom - known_fom):.4f}")

#genetic algorithm
import numpy as np
import random

def initialize_population(pop_size=50):
    """Generate random binary configurations (20x20)."""
    return np.random.randint(0, 2, size=(pop_size, 20, 20, 1)).astype(np.float32)

def evaluate(configs, model):
    """Predict FOMs for configurations using the trained model."""
    return model.predict(configs, verbose=0).flatten()

def select_parents(configs, foms, n_parents=10):
    """Select top n_parents configurations based on FOM."""
    sorted_indices = np.argsort(foms)[-n_parents:]  # Best have highest FOM
    return configs[sorted_indices], foms[sorted_indices]

def mutate(config, mutation_rate):
    """Apply mutations with given rate (in %)."""
    mask = np.random.rand(*config.shape) < (mutation_rate / 100)
    mutated = np.where(mask, 1 - config, config)  # Flip bits where mask=True
    return mutated

def generate_children(parents, parent_foms, children_per_parent=5):
    """Generate children with mutation rates inversely proportional to parent FOM rank."""
    children = []
    ranks = np.argsort(np.argsort(parent_foms))  # Rank 0=worst, 9=best
    for i, parent in enumerate(parents):
        # Scale mutation rate from 30% (worst) to 1% (best)
        mutation_rate = 30 - (29 * ranks[i] / (len(parents) - 1))
        for _ in range(children_per_parent):
            child = mutate(parent, mutation_rate)
            children.append(child)
    return np.array(children)

# def genetic_algorithm(model, target_fom=0.999, max_generations=10000):
#     """Run GA until target FOM is reached."""
#     population = initialize_population(50)
#     best_config = None
#     best_foms = []

#     for gen in range(max_generations):
#         # Evaluate current population
#         foms = evaluate(population, model)
#         best_fom = np.max(foms)
#         best_foms.append(best_fom)

#         print(f"Gen {gen}: Best FOM = {best_fom:.4f}")

#         # Check termination condition
#         if best_fom >= target_fom:
#             print(f"\nTarget FOM {target_fom} reached at generation {gen}!")
#             break

#         # Selection and reproduction
#         parents, parent_foms = select_parents(population, foms, 10)
#         children = generate_children(parents, parent_foms, 5)

#         # New population = parents + children
#         population = np.concatenate([parents, children])

#     return best_foms


def genetic_algorithm_with_tracking(model, target_fom=1, max_generations=1000):
    """Run GA and track best configuration."""
    population = initialize_population(200)
    best_foms = []
    best_configs = []  # Stores (config, fom) tuples

    for gen in range(max_generations):
        # Evaluate current population
        foms = evaluate(population, model)
        current_best_idx = np.argmax(foms)
        current_best_config = population[current_best_idx]
        current_best_fom = foms[current_best_idx]

        # Store best of generation
        best_foms.append(current_best_fom)
        best_configs.append((current_best_config.copy(), current_best_fom))

        print(f"\nGeneration {gen}: Best FOM = {current_best_fom:.4f}")
        print("Best configuration (first 5x5 corner):")
        print(current_best_config[:5, :5, 0])  # Print top-left corner

        # Termination check
        if current_best_fom >= target_fom:
            print(f"\n‚úÖ Target FOM {target_fom} reached!")

        # Selection and reproduction
        parents, parent_foms = select_parents(population, foms, 10)
        children = generate_children(parents, parent_foms, 5)
        population = np.concatenate([parents, children])

    # Return full history and final best config
    return best_foms, best_configs

# run algorithm
# Run and visualize
best_foms, best_configs = genetic_algorithm_with_tracking(model)

# Access final best config
final_best_config, final_best_fom = best_configs[-1]
print("\nüèÜ Final best configuration (full):")
print(final_best_config[:, :, 0])  # Print full 20x20 matrix